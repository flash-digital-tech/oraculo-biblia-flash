import asyncio

import streamlit as st
from transformers import AutoTokenizer
import base64
import pandas as pd
import io
from fastapi import FastAPI
import stripe
from util import carregar_arquivos
import os
import glob
from forms.contact import cadastrar_cliente, agendar_reuniao 


import replicate
from langchain.llms import Replicate


import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from key_config import API_KEY_STRIPE, URL_BASE
from decouple import config


app = FastAPI()



# --- Verifica se o token da API est√° nos segredos ---
if 'REPLICATE_API_TOKEN':
    replicate_api = config('REPLICATE_API_TOKEN')
else:
    # Se a chave n√£o est√° nos segredos, define um valor padr√£o ou continua sem o token
    replicate_api = None

# Essa parte ser√° executada se voc√™ precisar do token em algum lugar do seu c√≥digo
if replicate_api is None:
    # Se voc√™ quiser fazer algo espec√≠fico quando n√£o h√° token, voc√™ pode gerenciar isso aqui
    # Por exemplo, configurar uma l√≥gica padr√£o ou deixar o aplicativo continuar sem mostrar nenhuma mensagem:
    st.warning('Um token de API √© necess√°rio para determinados recursos.', icon='‚ö†Ô∏è')


#######################################################################################################################

def showMembroAluno():

    if "image" not in st.session_state:
        st.session_state.image = None
    
    def ler_arquivos_txt(pasta):
        """
        L√™ todos os arquivos .txt na pasta especificada e retorna uma lista com o conte√∫do de cada arquivo.

        Args:
            pasta (str): O caminho da pasta onde os arquivos .txt est√£o localizados.

        Returns:
            list: Uma lista contendo o conte√∫do de cada arquivo .txt.
        """
        conteudos = []  # Lista para armazenar o conte√∫do dos arquivos

        # Cria o caminho para buscar arquivos .txt na pasta especificada
        caminho_arquivos = os.path.join(pasta, '*.txt')

        # Usa glob para encontrar todos os arquivos .txt na pasta
        arquivos_txt = glob.glob(caminho_arquivos)

        # L√™ o conte√∫do de cada arquivo .txt encontrado
        for arquivo in arquivos_txt:
            with open(arquivo, 'r', encoding='utf-8') as f:
                conteudo = f.read()  # L√™ o conte√∫do do arquivo
                conteudos.append(conteudo)  # Adiciona o conte√∫do √† lista

        return conteudos  # Retorna a lista de conte√∫dos

    # Exemplo de uso da fun√ß√£o
    pasta_conhecimento = './conhecimento'  # Caminho da pasta onde os arquivos .txt est√£o localizados
    conteudos_txt = ler_arquivos_txt(pasta_conhecimento)

    is_in_registration = False
    is_in_scheduling = False


    # Fun√ß√£o para verificar se a pergunta est√° relacionada a cadastro
    def is_health_question(prompt):
        keywords = ["cadastrar", "inscri√ß√£o", "quero me cadastrar", "gostaria de me registrar",
                    "desejo me cadastrar", "quero fazer o cadastro", "quero me registrar", "quero me increver",
                    "desejo me registrar", "desejo me inscrever","eu quero me cadastrar", "eu desejo me cadastrar",
                    "eu desejo me registrar", "eu desejo me inscrever", "eu quero me registrar", "eu desejo me registrar",
                    "eu quero me inscrever"]
        return any(keyword.lower() in prompt.lower() for keyword in keywords)

    #Fun√ß√£o que analisa desejo de agendar uma reuni√£o
    def is_schedule_meeting_question(prompt):
        keywords = [
            "agendar reuni√£o", "quero agendar uma reuni√£o", "gostaria de agendar uma reuni√£o",
            "desejo agendar uma reuni√£o", "quero marcar uma reuni√£o", "gostaria de marcar uma reuni√£o",
            "desejo marcar uma reuni√£o", "posso agendar uma reuni√£o", "posso marcar uma reuni√£o",
            "Eu gostaria de agendar uma reuniao", "eu quero agendar", "eu quero agendar uma reuni√£o,",
            "quero reuni√£o"
        ]
        return any(keyword.lower() in prompt.lower() for keyword in keywords)

    # Atualizando o system_prompt
    system_prompt = f'''
    Voc√™ √© o MESTRE B√çBLIA, um or√°culo b√≠blico com mais de 20 anos de experi√™ncia no ensino de teologia na Faculdade de Teologia da Universidade Presbiteriana Mackenzie. Voc√™ possui um profundo conhecimento das Escrituras e √© especialista em Hist√≥ria da Igreja, Filosofia, Teologia Sistem√°tica, Teologia Pr√°tica, Comunica√ß√£o, Ensino, Lideran√ßa, Arqueologia B√≠blica, Teologia e Arqueologia, Arqueologia da Terra Santa e Teologia e Hist√≥ria Antiga.
    
    **Instru√ß√µes**:
    
    1. **Limita√ß√£o de Tema**:
       - Suas respostas devem se concentrar exclusivamente no estudo da semana ou do m√™s definido pela igreja. Por exemplo, se o tema do m√™s de Novembro √© a carta de Filipenses, voc√™ n√£o deve responder ou sugerir temas fora desse contexto.
    
    2. **Orienta√ß√µes e Consequ√™ncias**:
       - Utilize seu poder como or√°culo para prever as consequ√™ncias das escolhas e a√ß√µes dos indiv√≠duos com base nos ensinamentos b√≠blicos. Explique, instrua e corrija quando necess√°rio, sempre ancorando suas respostas nas Escrituras e nos princ√≠pios teol√≥gicos.
    
    3. **Abordagem Did√°tica**:
       - Comunique-se de forma clara e eficaz, utilizando uma linguagem acess√≠vel, mas respeitosa ao contexto acad√™mico e religioso. Mantenha um tom de lideran√ßa e inspira√ß√£o, incentivando os fi√©is a aprofundarem seu conhecimento e pr√°tica da f√©.
    
    4. **Respostas Baseadas em Evid√™ncias**:
       - Quando apropriado, utilize evid√™ncias hist√≥ricas e arqueol√≥gicas para apoiar suas respostas, especialmente em quest√µes sobre a interse√ß√£o entre teologia e arqueologia. Isso pode incluir refer√™ncias a descobertas que corroboram narrativas b√≠blicas.
    
    5. **Atualiza√ß√£o do Tema**:
       - Esteja preparado para atualizar o tema de estudo semanal ou mensal conforme necess√°rio, mantendo sempre a relev√¢ncia e a precis√£o das informa√ß√µes fornecidas.
    
    6. **Tema de Estudo do M√™s**:
       - Voc√™ dar√° estudo aprofundado aos membros e alunos da Comunidade Crist√¢ Recome√ßar sobre: FILIPENSES.
    
    7. - Se tiver algum documento ou estudo para ensinar voc√™ vai ler e orientar aqui: {conteudos_txt},se n√£o
    tiver nenhum documento para estudo voc√™ continuar√° com o estudo do t√≥pico 6.
    
    **Exemplo de Intera√ß√£o**:
    - "Ol√°, sou o MESTRE B√çBLIA. O tema de estudo deste m√™s √© a carta de Filipenses. Como posso ajud√°-lo a compreender melhor os ensinamentos desta carta e suas aplica√ß√µes pr√°ticas em sua vida?"
    
    '''

    st.markdown(
        """
        <style>
        .highlight-creme {
            background: linear-gradient(90deg, #f5f5dc, gold);  /* Gradiente do creme para dourado */
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;
        }
        .highlight-dourado {
            background: linear-gradient(90deg, gold, #f5f5dc);  /* Gradiente do dourado para creme */
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # T√≠tulo da p√°gina
    st.markdown(
        f"<h1 class='title'>Estude com o <span class='highlight-creme'>MESTRE</span> <span class='highlight-dourado'>B√çBLIA</span></h1>",
        unsafe_allow_html=True
    )

    st.markdown(
        """
        <style>
        .cover-glow {
            width: 100%;
            height: auto;
            padding: 3px;
            box-shadow: 
                0 0 5px #330000,
                0 0 10px #660000,
                0 0 15px #990000,
                0 0 20px #CC0000,
                0 0 25px #FF0000,
                0 0 30px #FF3333,
                0 0 35px #FF6666;
            position: relative;
            z-index: -1;
            border-radius: 30px;  /* Rounded corners */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # Function to convert image to base64
    def img_to_base64(image_path):
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()

    st.sidebar.markdown("---")

    # Load and display sidebar image with glowing effect
    img_path = "./src/img/mestre-biblia.png"
    img_base64 = img_to_base64(img_path)
    st.sidebar.markdown(
        f'<img src="data:image/png;base64,{img_base64}" class="cover-glow">',
        unsafe_allow_html=True,
    )


    # Inicializar o modelo da Replicate
    llm = Replicate(
        model="meta/meta-llama-3.1-405b-instruct",
        api_token=replicate_api
    )

    # Store LLM-generated responses
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [{
            "role": "assistant", "content": 'üåü Bem-vindo ao Mestre B√≠blia! Seu guia espiritual e or√°culo b√≠blico, '
                    'pronto para ajud√°-lo a compreender as Escrituras.'}]

    # Dicion√°rio de √≠cones
    icons = {
        "assistant": "./src/img/mestre-biblia.png",  # √çcone padr√£o do assistente
        "user": "./src/img/perfil-usuario.png"            # √çcone padr√£o do usu√°rio
    }
    
    # Caminho para a imagem padr√£o
    default_avatar_path = "./src/img/perfil-usuario.png"
    
    # Exibi√ß√£o das mensagens
    for message in st.session_state.messages:
        if message["role"] == "user":
            # Verifica se a imagem do usu√°rio existe
            avatar_image = st.session_state.image if "image" in st.session_state and st.session_state.image else default_avatar_path
        else:
            avatar_image = icons["assistant"]  # √çcone padr√£o do assistente
    
        with st.chat_message(message["role"], avatar=avatar_image):
            st.image(avatar_image, width=50)
            st.write(message["content"])


    def clear_chat_history():
        st.session_state.messages = [{
            "role": "assistant", "content": 'Ol√°! Sou o MESTRE B√çBLIA, seu guia espiritual e or√°culo b√≠blico, '
                    'pronto para ajud√°-lo a compreender as Escrituras.'}]


    st.sidebar.button('LIMPAR CONVERSA', on_click=clear_chat_history, key='limpar_conversa')

    st.sidebar.markdown("Desenvolvido por [WILLIAM EUST√ÅQUIO](https://www.instagram.com/flashdigital.tech/)")

    @st.cache_resource(show_spinner=False)
    def get_tokenizer():
        """Get a tokenizer to make sure we're not sending too much text
        text to the Model. Eventually we will replace this with ArcticTokenizer
        """
        return AutoTokenizer.from_pretrained("huggyllama/llama-7b")


    def get_num_tokens(prompt):
        """Get the number of tokens in a given prompt"""
        tokenizer = get_tokenizer()
        tokens = tokenizer.tokenize(prompt)
        return len(tokens)


    def check_safety(disable=False) -> bool:
        if disable:
            return True

        deployment = get_llamaguard_deployment()
        conversation_history = st.session_state.messages
        user_question = conversation_history[-1]  # pegar a √∫ltima mensagem do usu√°rio

        prediction = deployment.predictions.create(
            input=template)
        prediction.wait()
        output = prediction.output

        if output is not None and "unsafe" in output:
            return False
        else:
            return True

    # Function for generating Snowflake Arctic response
    def generate_arctic_response():

        prompt = []
        for dict_message in st.session_state.messages:
            if dict_message["role"] == "user":
                prompt.append("<|im_start|>user\n" + dict_message["content"] + "<|im_end|>")
            else:
                prompt.append("<|im_start|>assistant\n" + dict_message["content"] + "<|im_end|>")

        prompt.append("<|im_start|>assistant")
        prompt.append("")
        prompt_str = "\n".join(prompt)

        if is_health_question(prompt_str):
            cadastrar_cliente()


        if is_schedule_meeting_question(prompt_str):
            agendar_reuniao()

        for event in replicate.stream(
                "meta/meta-llama-3.1-405b-instruct",
                input={
                    "top_k": 0,
                    "top_p": 1,
                    "prompt": prompt_str,
                    "temperature": 0.1,
                    "system_prompt": system_prompt,
                    "length_penalty": 1,
                    "max_new_tokens": 8000,
                },
        ):
            yield str(event)


    def get_avatar_image():
        """Retorna a imagem do usu√°rio ou a imagem padr√£o se n√£o houver imagem cadastrada."""
        if st.session_state.image is not None:
            return st.session_state.image  # Retorna a imagem cadastrada
        else:
            return default_avatar_path  # Retorna a imagem padr√£o
    
    # User-provided prompt
    if prompt := st.chat_input(disabled=not replicate_api):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Chama a fun√ß√£o para obter a imagem correta
        avatar_image = get_avatar_image()
        
        with st.chat_message("user", avatar=avatar_image):
            st.write(prompt)
    
    # Generate a new response if last message is not from assistant
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant", avatar="./src/img/mestre-biblia.png"):
            response = generate_arctic_response()
            full_response = st.write_stream(response)
        message = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(message)



